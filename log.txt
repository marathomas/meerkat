18.05.2020, Mon

Literature review, trying to understand spectrograms.

19.05.2020, Tue

Played around with the audio files, chunking them and creating spectrograms to understand this a little bit better, using mostly librosa package.

20.05.2020, Wed

Thoroughly read the Gentner paper, now feel like I have a somewhat solid understanding of it. Installed the AVGN package from Sainsberg in google colab and got familiar with it. However, I need the audio files now! Call with Ari, Marie, Frants and Marta in the evening, took notes on paper.
Short version: Marta also has lots of meerkat call data (labelled), and she's interested in unsupervised approaches, but no one in her group is really doing it. Lots of questions one could ask, f.e. whether unsupervised clustering can differentiate predator-specific calls etc. Everybody talks about neural nets all the time, which got me quite confused but at the end of the call I could clarify that the authors actually present a dimensionality reduction using UMAP before clustering and it would make sense that I do the same.

25.05.2020, Mon

Ari sent me a folder containing 52 CSV files, they contain the labels of 52 wav files. Following her recommendation, I wrote a script to find the correct wav files for these CSV files (out of ~1030 wav files in total). They can be easily matched via pattern matching with an identifier as the .wav files are labeled identifier.wav, and the csv files identifier+labels+initials.csv. Wrote a bash script to do that and copy the relevant audio files into a separate folder.

Following problems were encountered:
1) Some CSV match to two wav files. The wav files are in different subdirectories and are either named identifier.wav oder identifier_labels.wav (or similar).
2) I found matching wavs for only 33 of the 52 CSV files. Need to check what's the problem here, if that is possible.

Need to print out the missing ones!
Haven't filtered SOUNDFOC etc yet.

26.05.2020, Tue

Got VPN access. Need to find a way to get datafiles from mycloud to colab... 
Other task: Segment the audio files according to the labels so that I have small chunks containing the vocalization only.
Kiran shared her preprocessing code with me. However, she is directly generating spectrograms, I just want to split the wav files. Need to talk to Ari if this is something to factor in? Need to understand her code better, many packages that I don't know of. I also wrote code to extract the start and stop time in milliseconds of the labelled sections in the video, but now realized there's a datatype called "datetime" in python which could simplify things for me so need to check it out.

- make dictionary with filename, wav.file-path and csv.file-path (--> ok, made data frame, called matching.txt)
 
CALL - Update (no updates from me, really, just witnessing how Marie helps Kiran out with the meerkat neural network code)

27.05.2020

- finished the select_file bash script to find corresponding wav and csv files
- write python script to chunk audios according to the labels in csv files
- converted python script to more functions, less analysis script, but I'm not sure it really helped, maybe I should make the functions more generalizable
- bought 200GB storage on google drive and uploaded all relevant wavs

Currently: select files locally using hard drive. Upload relevant files to google drive. Use colab for preprocessing and analyses.

TODOs:
- @Ari: why is the duration of calls so short!! milliseconds!? doesn't make sense
--> geklärt, sind so kurz
- improve code, revisit some style guidelines and python data structure info
--> nochmal über tuples, lists etc gelesen, keine guten Ideen den code zu verbessern, außer evtl mehr Fehlermeldungen und Funktionen robuster. aber lohnt sich kaum, da ich die nur so wenig benutze.
- put project on GitHub (version control!)
- generate JSON metafiles for audios

28.05.2020
- tried to run audio chunking with ALL files. Always errors. Can fix some, for example empty label data frames if no calls were labeled in the file. New problem: one file is in matching.txt but not in the wav-folder: HM_VCVM001_HMB_AUDIO_R08_ file_2_(2017_08_03-06_44_59)_ASWMUX221153.wav
Die ist irgendwie abgeschnitten im cleaned_wav.txt ..!?? Versteh nicht wieso. Habs's manuell hinzugefügt. 
Kommen jetzt Fehler beim add_ms, weiß auch warum: das format ist teilweise nur %min:%s.%ms statt %h:%min:%s.%ms. Neu schreiben damit das alles handeln kann, ist gar nicht so einfach. Kommt zB vor bei HM_RT_R12_file_5_(2017_08_24-06_44_59)_ASWMUX221102 im gesamten oberen Teil des Files.

TODOs:
- file select script neu schreiben
- make chunking more generalizable using date time -> OK

29.05.2020
- rewrote code using datetime, making it less vulnerable to different time formats
- makes everything shorter, too
- tried to connect colab with GitHub (following this tutorial: https://zerowithdot.com/colab-github-workflow/, GitHub meerkat repo token:
4328d4676781f7762142ddd7bad8f2684e1b3c11), but it didn't work and I gave up

Now the next irregularity: column name is called X...Name in file HM_HTB_R14_20170903-20170908_file_2_(2017_09_03-05_44_59)_ASWMUX221052.wav. Rewrote script to identify name column. Let's see about the other unknown irregularities.

So far - nothing. Created all audio chunks, except for one file, where there are no labelled calls: HM_VHMM002_HRT_AUDIO_R09_file_5_(2017_08_06-06_44_59)_ASWMUX221110_label.csv

TODO(old):
- file select script neu schreiben - OK!
- put project on GitHub (version control!) (try again) - BAM! Success!!
- generate JSON metafiles for audios
TODO (new):
- re-do everything with 2019 audios, Ari has sent me the label files (but first, rewrite file select script) - OK, don't have the wavs, need to get them from cloud.
- maybe consider doing everything a bit more my own way, as it seems cooking the AVGN recipe is not as easy as it initially seemed. It's not really a package. Maybe directly generate spectrograms? instead of chunking the audio. Check out this notebook: https://raw.githubusercontent.com/timsainb/AVGN/master/notebooks/birdsong/bengalese_finch_example/1.0-segment-song-from-wavs.ipynb
- review the literature and methods (spectrogramming etc to find out more, do it more as Kiran did)

01.06.2020

tried to select all audios for 2019, but wasn't able to match any. Also had to rewrite file_select as the block comments didn't work (I don't get why!??) Code is executed despite the block comment signal. I think I should rewrite the file_matching by cutting all from the csv filename after the last "number", that should be possible? would make things much easier. Also should consider writing it in Python..? And maybe I should adapt my python script so that it doesn't contain bash commands!? because this probably only works in Jupiter notebook. Should maybe get inspiration from Kiran's script? 
Or just stay with bash for now.

02.06.2020

Re-wrote file_select so that it removes the "Label_XY" whatsoever from the csv files and then searches for the corresponding wavs, makes things a lot easier. Also organized the Unsup_clustering folder, removed old scripts and the new script is now called file_selector.

2017: ./file_selector -csv /Users/marathomas/Documents/Bioinformatik/Masterarbeit/Unsup_clustering/Data/2017_labels -wav /Volumes/MaraMeerkat
2019: ./file_selector -csv /Users/marathomas/Documents/Bioinformatik/Masterarbeit/Unsup_clustering/Data/2019_labels -wav /Volumes/MaraMeerkat

I don't have the 2019 wavs though.

TODO(old):
- generate JSON metafiles for audios
- maybe consider doing everything a bit more my own way, as it seems cooking the AVGN recipe is not as easy as it initially seemed. It's not really a package. Maybe directly generate spectrograms? instead of chunking the audio. Check out this notebook: https://raw.githubusercontent.com/timsainb/AVGN/master/notebooks/birdsong/bengalese_finch_example/1.0-segment-song-from-wavs.ipynb
- review the literature and methods (spectrogramming etc to find out more, do it more as Kiran did)
TODO (new):
- re-do everything with 2019 audios from Cloud (connect to Cloud..!?) 
- rewrite Jupiter notebook? make it Python script (no bash commands!) -> removed all of the bash commands (I think)
- understand the marmoset pipeline up to UMAP (3 notebooks)


Remove bad quality sounds. Ari sent me an excel sheet containing some comments about the quality of the sounds, because some are difficult to distinguish from noise. Labelling is always done for one hour of audio. Altogether,there are three hours where quality is labelled as "poor". By date and by meerkat identifier, I searched the corresponding filenames and identified two:
- HM_VHMM007_LT_AUDIO_R11_file_5_(2017_08_06-06_44_59)_ASWMUX221163
- HM_VHMM006_RT_AUDIO_R14_file_5_(2017_08_06-06_44_59)_ASWMUX221052

These I should probably remove, because they won't be usable.
->removed from drive segmented_audios

AVGN Marmoset seems similar to meerkat data: 
"This dataset has:
A number of WAVs where naming convention stores the individuals vocalizing
Corresponding .mat files with the timing of each phee/call and the individual making the vocalization.
This notebook extracts periods of vocalization into new WAV files, and creates a corresponding JSON and TextGrid for each WAV with annotation information"
avgn_paper/notebooks/01.0-custom_parsing/01.0-marmoset-prepare-dataset.ipynb

Maybe go along this one? Seem to capture noise (get_noise..), which helps for later analysis.

Maybe not chunk audios, but directly create spectrograms based on start and stop times, with some padding and with some noise !?
-> check Kiran's code.

Now trying to understand Sainsberg code, using Marmoset dataset as example. I think these three notebooks should show the steps until UMAP projection.

avgn_paper/notebooks/01.0-custon-parsing/01.0-marmoset-prepare-dataset.ipynb
avgn_paper/notebooks/02.0-make-syllable_df/0.0-marmoset-create-syllable-df.ipynb
avgn_paper/notebooks/02.1-poject-UMAP/4.0-marmoset-dataset-umap.ipynb

03.06.2020

- Downloading the 2019 data to the meerkat drive so that I can select the wavs and upload to google drive
- understood and annotated 01.0-marmoset-prepare-dataset.ipynb, should be able to adjust the script to my data or write my own code to create the same output. Probably good to create the exact same objects etc so that I can seamlessly integrate with the rest of the AVGN pipeline. From here on, the next steps are more general.

I can't find proper definitions of concepts used in the Sainsberg script. For example, what exactly defines a bout vs. a syllable vs. a call vs. a pulse vs. a vocalization !?

Sometimes called this, sometimes called that, very confusing.

TODO(old):
- generate JSON metafiles for audios
- review the literature and methods (spectrogramming etc to find out more, do it more as Kiran did)
- re-do everything with 2019 audios from Cloud (connect to Cloud..!?) 
- understand the marmoset pipeline up to UMAP (3 notebooks) -> finished notebook 1
TODO (new):
- find out what unnamed column in labels is and why it is created (fix!)




